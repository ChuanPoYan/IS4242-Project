{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH1guBbELht6"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MFlRBTfOQ97",
    "outputId": "5348e19c-ff9a-4b43-a90b-03846bb2acda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/PoYan1/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/PoYan1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/PoYan1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/PoYan1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/PoYan1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import random\n",
    "import re\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import load_npz, save_npz\n",
    "from sklearn import utils\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow.keras.layers as layers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import umap\n",
    "import torch\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Download required nltk data\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2S9WQBkLht8"
   },
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "id": "5vDx5OsjXeIz",
    "outputId": "102f919a-e4eb-4e84-94b7-b9956a3d18c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preview</th>\n",
       "      <th>content</th>\n",
       "      <th>num_shares_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kampong gelam bazaar 2023 light projection sul...</td>\n",
       "      <td>there elevated mezzanine seating area fairy li...</td>\n",
       "      <td>kampong gelam bazaar 2023 every ramadan muslim...</td>\n",
       "      <td>Low Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>headrock vr virtual reality theme park sentosa...</td>\n",
       "      <td>the hyperrealistic virtual skyscraper game sen...</td>\n",
       "      <td>headrock vr singapore looking fun activity ide...</td>\n",
       "      <td>Low Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30 free steam game add library coop shooter ho...</td>\n",
       "      <td>thankfully great game require portion paycheck...</td>\n",
       "      <td>free steam game play home gamers singapore agr...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marina south pier ferry ride southern island r...</td>\n",
       "      <td>if looking outdoor adventure beyond location l...</td>\n",
       "      <td>marina south pier guide looking outdoor advent...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tayo station massive indoor playground multi z...</td>\n",
       "      <td>fan tayo little bus right way please</td>\n",
       "      <td>tayo station downtown east kid want play care ...</td>\n",
       "      <td>Low Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>12 best lobangs july 2019 1for1 mookata buffet...</td>\n",
       "      <td>enjoy july 2019 handful lobangs stretch dollar...</td>\n",
       "      <td>best deal singapore july 2019 karaoke manekine...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>5 bathroom fitting hdb singapore give 5star sh...</td>\n",
       "      <td>how make bathroom luxe would taking extralong ...</td>\n",
       "      <td>bathroom fitting hbd singapore hansgrohe long ...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>capri fraser new igfriendly hotel cbd gym pool...</td>\n",
       "      <td>the new capri fraser china square staycays res...</td>\n",
       "      <td>the new capri fraser china square staycays res...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>20 thing july 2019 50cents fest street fighter...</td>\n",
       "      <td>thing july 2019 like half year gone smack midd...</td>\n",
       "      <td>thing july 2019 like half year gone smack midd...</td>\n",
       "      <td>High Shares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>6 useful skillsfuture course singaporean learn...</td>\n",
       "      <td>skillsfuture course singapore know officially ...</td>\n",
       "      <td>skillsfuture course singapore know officially ...</td>\n",
       "      <td>Average Shares</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3992 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     kampong gelam bazaar 2023 light projection sul...   \n",
       "1     headrock vr virtual reality theme park sentosa...   \n",
       "2     30 free steam game add library coop shooter ho...   \n",
       "3     marina south pier ferry ride southern island r...   \n",
       "4     tayo station massive indoor playground multi z...   \n",
       "...                                                 ...   \n",
       "3987  12 best lobangs july 2019 1for1 mookata buffet...   \n",
       "3988  5 bathroom fitting hdb singapore give 5star sh...   \n",
       "3989  capri fraser new igfriendly hotel cbd gym pool...   \n",
       "3990  20 thing july 2019 50cents fest street fighter...   \n",
       "3991  6 useful skillsfuture course singaporean learn...   \n",
       "\n",
       "                                                preview  \\\n",
       "0     there elevated mezzanine seating area fairy li...   \n",
       "1     the hyperrealistic virtual skyscraper game sen...   \n",
       "2     thankfully great game require portion paycheck...   \n",
       "3     if looking outdoor adventure beyond location l...   \n",
       "4                  fan tayo little bus right way please   \n",
       "...                                                 ...   \n",
       "3987  enjoy july 2019 handful lobangs stretch dollar...   \n",
       "3988  how make bathroom luxe would taking extralong ...   \n",
       "3989  the new capri fraser china square staycays res...   \n",
       "3990  thing july 2019 like half year gone smack midd...   \n",
       "3991  skillsfuture course singapore know officially ...   \n",
       "\n",
       "                                                content  num_shares_bin  \n",
       "0     kampong gelam bazaar 2023 every ramadan muslim...      Low Shares  \n",
       "1     headrock vr singapore looking fun activity ide...      Low Shares  \n",
       "2     free steam game play home gamers singapore agr...     High Shares  \n",
       "3     marina south pier guide looking outdoor advent...     High Shares  \n",
       "4     tayo station downtown east kid want play care ...      Low Shares  \n",
       "...                                                 ...             ...  \n",
       "3987  best deal singapore july 2019 karaoke manekine...     High Shares  \n",
       "3988  bathroom fitting hbd singapore hansgrohe long ...     High Shares  \n",
       "3989  the new capri fraser china square staycays res...     High Shares  \n",
       "3990  thing july 2019 like half year gone smack midd...     High Shares  \n",
       "3991  skillsfuture course singapore know officially ...  Average Shares  \n",
       "\n",
       "[3992 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/SmartLocal/smartlocal_text.csv')\n",
    "rand_state = 42\n",
    "text_cols = ['title', 'preview', 'content']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JiTk9jeRT6hN"
   },
   "outputs": [],
   "source": [
    "def remove_start_end(text):\n",
    "    text = re.sub('Follow us on Telegram for the latest updates: https://t.me/TSLMedia ', '', text)\n",
    "    text = re.sub(\" Get more stories like this. Drop us your email so you won't miss the latest news. Name Your Name Email Your Email Subscribe\", '', text)\n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"Converts a text into its lemmatized form.\"\"\"\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return ' '.join([wnl.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_punctuations(text: str) -> str:\n",
    "    \"\"\"Removes punctuations and keeps all alphanumerical text.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_tags(text):\n",
    "    # removes social media tags\n",
    "    return re.sub('@\\w+', '', text.lower())\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    \"\"\"Removes stopwords from text.\"\"\"\n",
    "    for s in stopwords:\n",
    "        pattern = f' {s} '\n",
    "        text = re.sub(pattern, ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "G6XQt-Z7Lht-"
   },
   "outputs": [],
   "source": [
    "# Default stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Extend stop words\n",
    "stop_words = ['cover image adapted from', 'cover image credits',\n",
    "              'cover image credit', 'image credits', 'image credit',\n",
    "              'image adapted from', 'photography by', 'ha', 'wa'] + stop_words\n",
    "\n",
    "# List of default stopwords with punctuations removed, to avoid issues of encoding\n",
    "stop_words_no_punc = [remove_punctuations(word) for word in stop_words]\n",
    "\n",
    "functions = [remove_start_end, remove_tags, contractions.fix, remove_punctuations, \n",
    "             lambda z: remove_stopwords(text=z, stopwords=stop_words), \n",
    "             lemmatize]\n",
    "mass_apply = lambda x: reduce(lambda y, f: f(y), functions, x)\n",
    "\n",
    "# Process title, preview and content\n",
    "for text_col in text_cols:\n",
    "    df[text_col] \\\n",
    "        .apply(mass_apply) \\\n",
    "        .to_csv(f'./dataset/{text_col}_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF-r0SJJmkaj"
   },
   "source": [
    "# dataset/text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MuhcrjlqLht_"
   },
   "outputs": [],
   "source": [
    "text_processed = {}\n",
    "for text_col in text_cols:\n",
    "    train_df, test_df = train_test_split(pd.read_csv(f'./dataset/{text_col}_processed.csv'), test_size=0.2, random_state=rand_state)\n",
    "    text_processed[text_col] = {'train': train_df.to_numpy(), 'test': test_df.to_numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6MPsJ5GELhuA"
   },
   "outputs": [],
   "source": [
    "def avg_word2vec(model, model_vocabs, tokenized_sentences, size):\n",
    "    try:\n",
    "        list_of_wv = [[model.wv[token] for token in sentence if token in model_vocabs] for sentence in tokenized_sentences]\n",
    "    except:\n",
    "        list_of_wv = [[model.get_vector(token) for token in sentence if token in model_vocabs] for sentence in tokenized_sentences]\n",
    "    \n",
    "    list_of_avg_wv = []\n",
    "    for wvs in list_of_wv:\n",
    "        wvs = np.array(wvs)\n",
    "        if len(wvs) > 0:\n",
    "            list_of_avg_wv.append(wvs.mean(axis=0))\n",
    "        else:\n",
    "            list_of_avg_wv.append(np.zeros(size, dtype=float))\n",
    "    return np.array(list_of_avg_wv)\n",
    "\n",
    "def tagged_document(post):\n",
    "    return TaggedDocument(words=post)\n",
    "\n",
    "def avg_doc2vec(model, tagged_docs, category_dict):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(category_dict[doc.tags], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return np.array(regressors)\n",
    "\n",
    "\n",
    "def bert_dist_embed(posts, tokenizer, model):\n",
    "    embedding_res = np.empty(shape=(0, 768))\n",
    "    for batch_no in trange(0, len(posts), 100):\n",
    "        tokenized = tokenizer(list(posts[batch_no:batch_no+100]), \n",
    "                              padding = True, \n",
    "                              truncation = True, \n",
    "                              return_tensors = \"pt\")\n",
    "    with torch.no_grad():\n",
    "        hidden = model(**tokenized)\n",
    "        \n",
    "    batch = hidden.last_hidden_state[:,0,:].cpu().detach().numpy()\n",
    "    embedding_res = np.append(embedding_res, batch, axis=0)\n",
    "    return embedding_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdZVf43BOkek"
   },
   "source": [
    "### 1. Trained Word2Vec (Skip-Gram, CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "U8DIRgflLhuA",
    "outputId": "c03bfb89-bed6-4bc4-c5e1-b96d5d9c8db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3193, 100) (799, 100) (3193, 100) (799, 100) (3193, 300) (799, 300) (3193, 100) (799, 100)\n",
      "(3193, 100) (799, 100) (3193, 100) (799, 100) (3193, 300) (799, 300) (3193, 100) (799, 100)\n",
      "(3193, 100) (799, 100) (3193, 100) (799, 100) (3193, 300) (799, 300) (3193, 100) (799, 100)\n"
     ]
    }
   ],
   "source": [
    "for text_col in text_cols:\n",
    "    train = [sentence[0].split() for sentence in text_processed[text_col]['train']]\n",
    "    test = [sentence[0].split() for sentence in text_processed[text_col]['test']]\n",
    "    \n",
    "    skip_gram_model = Word2Vec(train, sg=1, min_count=1)\n",
    "    skipgram_wordpool = set(skip_gram_model.wv.index_to_key)\n",
    "    X_train_sg = avg_word2vec(skip_gram_model, skipgram_wordpool, train, 100)\n",
    "    X_test_sg = avg_word2vec(skip_gram_model, skipgram_wordpool, test, 100)\n",
    "    pd.DataFrame(X_train_sg).to_csv(f'./dataset/text_embedding/{text_col}/emb_sg_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_sg).to_csv(f'./dataset/text_embedding/{text_col}/emb_sg_test.csv', index=False)\n",
    "    \n",
    "    cbow_model = Word2Vec(train, sg=0, min_count=1)\n",
    "    dbow_wordpool = set(cbow_model.wv.index_to_key)\n",
    "    X_train_cbow = avg_word2vec(cbow_model, dbow_wordpool, train, 100)\n",
    "    X_test_cbow = avg_word2vec(cbow_model, dbow_wordpool, test, 100)\n",
    "    pd.DataFrame(X_train_cbow).to_csv(f'./dataset/text_embedding/{text_col}/emb_cbow_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_cbow).to_csv(f'./dataset/text_embedding/{text_col}/emb_cbow_test.csv', index=False)\n",
    "    \n",
    "    google_file = './dataset/Pretrained Embedding Model/GoogleNews-vectors-negative300.bin'\n",
    "    google_model = KeyedVectors.load_word2vec_format(google_file, binary=True)\n",
    "    google_wordpool = set(google_model.index_to_key)\n",
    "    X_train_ggl = avg_word2vec(google_model, google_wordpool, train, 300)\n",
    "    X_test_ggl = avg_word2vec(google_model, google_wordpool, test, 300)\n",
    "    pd.DataFrame(X_train_ggl).to_csv(f'./dataset/text_embedding/{text_col}/emb_ggl_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_ggl).to_csv(f'./dataset/text_embedding/{text_col}/emb_ggl_test.csv', index=False)\n",
    "    \n",
    "    glove_file = './dataset/Pretrained Embedding Model/glove.6B.100d.txt'\n",
    "    glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "    glove_wordpool = set(glove_model.index_to_key)\n",
    "    X_train_glove = avg_word2vec(glove_model, glove_wordpool, train, 100)\n",
    "    X_test_glove = avg_word2vec(glove_model, glove_wordpool, test, 100)\n",
    "    pd.DataFrame(X_train_glove).to_csv(f'./dataset/text_embedding/{text_col}/emb_glove_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_glove).to_csv(f'./dataset/text_embedding/{text_col}/emb_glove_test.csv', index=False)\n",
    "    \n",
    "    print(X_train_sg.shape, X_test_sg.shape, X_train_cbow.shape, X_test_cbow.shape, X_train_ggl.shape, X_test_ggl.shape, X_train_glove.shape, X_test_glove.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhshu1WLw4Z2"
   },
   "source": [
    "### 2. Trained Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pxwJ0UDRLhuB",
    "outputId": "5d970df4-ac53-4d35-c98e-29fcef6f6647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words='qoo10 deal 099 brand like koi shihlin plus chance score new iphone 14', tags=['Low Shares'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=rand_state)\n",
    "# represent each sentence as a TaggedDocument containing 2 parameters, words=tokenized_sentence and tag=label\n",
    "train_r = pd.DataFrame({'num_shares_bin': train_df['num_shares_bin'].to_numpy(), 'title': train_df['title'].to_numpy()}, columns=['num_shares_bin', 'title'])\n",
    "test_r = pd.DataFrame({'num_shares_bin': test_df['num_shares_bin'].to_numpy(), 'title': test_df['title'].to_numpy()}, columns=['num_shares_bin', 'title'])\n",
    "train_tagged = train_r.apply(lambda r: TaggedDocument(words=r['title'], tags=[r['num_shares_bin']]), axis=1)\n",
    "test_tagged = test_r.apply(lambda r: TaggedDocument(words=r['title'], tags=[r['num_shares_bin']]), axis=1)\n",
    "\n",
    "# visualize a TaggedDocument\n",
    "train_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JRD9BR1Rw5KY",
    "outputId": "95c6ba7a-c03f-4b7c-a9fc-54d25f6b4c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3193, 300) (799, 300) (3193, 300) (799, 300) (3193, 600) (799, 600)\n",
      "(3193, 300) (799, 300) (3193, 300) (799, 300) (3193, 600) (799, 600)\n",
      "(3193, 300) (799, 300) (3193, 300) (799, 300) (3193, 600) (799, 600)\n"
     ]
    }
   ],
   "source": [
    "# use multiple cores\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Define the category dictionary\n",
    "category_dict = {'Low Shares': 0, 'Average Shares': 1, 'High Shares': 2}\n",
    "\n",
    "for text_col in text_cols:\n",
    "    train = [sentence[0].split() for sentence in text_processed[text_col]['train']]\n",
    "    test = [sentence[0].split() for sentence in text_processed[text_col]['test']]\n",
    "    \n",
    "    # represent each sentence as a TaggedDocument containing 2 parameters, words=tokenized_sentence and tag=label\n",
    "    train_r = pd.DataFrame({'num_shares_bin': train_df['num_shares_bin'].astype(str), text_col: train}, columns=['num_shares_bin', text_col])\n",
    "    test_r = pd.DataFrame({'num_shares_bin': test_df['num_shares_bin'].astype(str), text_col: test}, columns=['num_shares_bin', text_col])\n",
    "    train_tagged = train_r.apply(lambda r: TaggedDocument(words=r[text_col], tags=r['num_shares_bin']), axis=1)\n",
    "    test_tagged = test_r.apply(lambda r: TaggedDocument(words=r[text_col], tags=r['num_shares_bin']), axis=1)\n",
    "\n",
    "    # implement Distributed Bag of Words (DBOW) (similar concept to skip-gram)\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_tagged.values, disable=True)])\n",
    "    for epoch in range(30):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values, disable=True)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "    # implement Distributed Memory (DM) (similar concept to CBOW)\n",
    "    model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "    model_dm.build_vocab([x for x in tqdm(train_tagged.values, disable=True)])\n",
    "    for epoch in range(30):\n",
    "        model_dm.train(utils.shuffle([x for x in tqdm(train_tagged.values, disable=True)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "        model_dm.alpha -= 0.002\n",
    "        model_dm.min_alpha = model_dm.alpha\n",
    "        \n",
    "    # dataset/text_embedding using dbow\n",
    "    X_train_dbow = avg_doc2vec(model_dbow, train_tagged, category_dict)\n",
    "    X_test_dbow = avg_doc2vec(model_dbow, test_tagged, category_dict)\n",
    "\n",
    "    # dataset/text_embedding using dm\n",
    "    X_train_dm = avg_doc2vec(model_dm, train_tagged, category_dict)\n",
    "    X_test_dm = avg_doc2vec(model_dm, test_tagged, category_dict)\n",
    "\n",
    "    # dataset/text_embedding by combining a paragraph vector from DBOW and DM to improve performance\n",
    "    model_dbow.wv.fill_norms(force=True)\n",
    "    model_dm.wv.fill_norms(force=True)\n",
    "    model_dbow_dm = ConcatenatedDoc2Vec([model_dbow, model_dm])\n",
    "    X_train_dbow_dm = avg_doc2vec(model_dbow_dm, train_tagged, category_dict)\n",
    "    X_test_dbow_dm = avg_doc2vec(model_dbow_dm, test_tagged, category_dict)\n",
    "    \n",
    "    # export to csv\n",
    "    pd.DataFrame(X_train_dbow).to_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_dbow).to_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_test.csv', index=False)\n",
    "    pd.DataFrame(X_train_dm).to_csv(f'./dataset/text_embedding/{text_col}/emb_dm_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_dm).to_csv(f'./dataset/text_embedding/{text_col}/emb_dm_test.csv', index=False)\n",
    "    pd.DataFrame(X_train_dbow_dm).to_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_dm_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_dbow_dm).to_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_dm_test.csv', index=False)\n",
    "    \n",
    "    print(X_train_dbow.shape, X_test_dbow.shape, X_train_dm.shape, X_test_dm.shape, X_train_dbow_dm.shape, X_test_dbow_dm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22Wq-c_H9B2D"
   },
   "source": [
    "### 3. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "01snimE09JzF",
    "outputId": "41ad29b2-1b07-454b-d0f8-cd7c298c5d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3193, 1423534) (799, 1423534)\n",
      "(3193, 1423534) (799, 1423534)\n",
      "(3193, 1423534) (799, 1423534)\n"
     ]
    }
   ],
   "source": [
    "for text_col in text_cols:\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (2, 2))\n",
    "    X_train_tfidf = vectorizer.fit_transform(map(lambda x: ' '.join(x), train))\n",
    "    X_test_tfidf = vectorizer.transform(map(lambda x: ' '.join(x), test))\n",
    "    \n",
    "    save_npz(f'./dataset/text_embedding/{text_col}/emb_tfidf_train.npz', X_train_tfidf)\n",
    "    save_npz(f'./dataset/text_embedding/{text_col}/emb_tfidf_test.npz', X_test_tfidf)\n",
    "\n",
    "    print(X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJAqxu-OaqWC"
   },
   "source": [
    "### 4. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qMNXN7tbb6B7",
    "outputId": "c11da92e-b321-447c-d159-8c15dffeed77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3193, 768) (799, 768)\n",
      "(3193, 768) (799, 768)\n",
      "(3193, 768) (799, 768)\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "for text_col in text_cols:\n",
    "    train = text_processed[text_col]['train'].reshape(-1)\n",
    "    test = text_processed[text_col]['test'].reshape(-1)\n",
    "\n",
    "    X_train_bert = embedder.encode(train)\n",
    "    X_test_bert = embedder.encode(test)\n",
    "    X_train_bert = pd.DataFrame(X_train_bert)\n",
    "    X_test_bert = pd.DataFrame(X_test_bert)\n",
    "    pd.DataFrame(X_train_bert).to_csv(f'./dataset/text_embedding/{text_col}/emb_bert_train.csv', index=False)\n",
    "    pd.DataFrame(X_test_bert).to_csv(f'./dataset/text_embedding/{text_col}/emb_bert_test.csv', index=False) \n",
    "    \n",
    "    print(X_train_bert.shape, X_test_bert.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xsGNb2LqGEN"
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dGJCJjZ8FP58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "length = 5\n",
    "\n",
    "# # word2vec: skip-gram\n",
    "X_train_sg = []\n",
    "X_test_sg = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_sg_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_sg_train.csv')\n",
    "    X_train_sg_text_col = pd.DataFrame(dim.fit_transform(X_train_sg_text_col))\n",
    "    X_train_sg.append(X_train_sg_text_col)\n",
    "    \n",
    "    X_test_sg_test_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_sg_test.csv')\n",
    "    X_test_sg_test_col = pd.DataFrame(dim.transform(X_test_sg_test_col))\n",
    "    X_test_sg.append(X_test_sg_test_col)\n",
    "X_train_sg = pd.concat(X_train_sg, axis=1)\n",
    "X_test_sg = pd.concat(X_test_sg, axis=1)\n",
    "    \n",
    "# word2vec: cbow\n",
    "X_train_cbow = []\n",
    "X_test_cbow = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_cbow_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_cbow_train.csv')\n",
    "    X_train_cbow_text_col = pd.DataFrame(dim.fit_transform(X_train_cbow_text_col))\n",
    "    X_train_cbow.append(X_train_cbow_text_col)\n",
    "    \n",
    "    X_test_cbow_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_cbow_test.csv')\n",
    "    X_test_cbow_text_col = pd.DataFrame(dim.transform(X_test_cbow_text_col))\n",
    "    X_test_cbow.append(X_test_cbow_text_col)\n",
    "X_train_cbow = pd.concat(X_train_cbow, axis=1)\n",
    "X_test_cbow = pd.concat(X_test_cbow, axis=1)\n",
    "\n",
    "# doc2vec: dbow\n",
    "X_train_dbow = []\n",
    "X_test_dbow = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_dbow_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_train.csv')\n",
    "    X_train_dbow_text_col = pd.DataFrame(dim.fit_transform(X_train_dbow_text_col))\n",
    "    X_train_dbow.append(X_train_dbow_text_col)\n",
    "    \n",
    "    X_test_dbow_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_test.csv')\n",
    "    X_test_dbow_text_col = pd.DataFrame(dim.transform(X_test_dbow_text_col))\n",
    "    X_test_dbow.append(X_test_dbow_text_col)\n",
    "X_train_dbow = pd.concat(X_train_dbow, axis=1)\n",
    "X_test_dbow = pd.concat(X_test_dbow, axis=1)\n",
    "\n",
    "# doc2vec: dm\n",
    "X_train_dm = []\n",
    "X_test_dm = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_dm_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dm_train.csv')\n",
    "    X_train_dm_text_col = pd.DataFrame(dim.fit_transform(X_train_dm_text_col))\n",
    "    X_train_dm.append(X_train_dm_text_col)\n",
    "    \n",
    "    X_test_dm_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dm_test.csv')\n",
    "    X_test_dm_text_col = pd.DataFrame(dim.transform(X_test_dm_text_col))\n",
    "    X_test_dm.append(X_test_dm_text_col)\n",
    "X_train_dm = pd.concat(X_train_dm, axis=1)\n",
    "X_test_dm = pd.concat(X_test_dm, axis=1)\n",
    "\n",
    "# doc2vec: dbow + dm\n",
    "X_train_dbow_dm = []\n",
    "X_test_dbow_dm = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_dbow_dm_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_dm_train.csv')\n",
    "    X_train_dbow_dm_text_col = pd.DataFrame(dim.fit_transform(X_train_dbow_dm_text_col))\n",
    "    X_train_dbow_dm.append(X_train_dbow_dm_text_col)\n",
    "    \n",
    "    X_test_dbow_dm_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_dbow_dm_test.csv')\n",
    "    X_test_dbow_dm_text_col = pd.DataFrame(dim.transform(X_test_dbow_dm_text_col))\n",
    "    X_test_dbow_dm.append(X_test_dbow_dm_text_col)\n",
    "X_train_dbow_dm = pd.concat(X_train_dbow_dm, axis=1)\n",
    "X_test_dbow_dm = pd.concat(X_test_dbow_dm, axis=1)\n",
    "\n",
    "# tf-idf\n",
    "X_train_tfidf = []\n",
    "X_test_tfidf = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_tfidf_text_col = load_npz(f'./dataset/text_embedding/{text_col}/emb_tfidf_train.npz')\n",
    "    X_train_tfidf_text_col = pd.DataFrame(dim.fit_transform(X_train_tfidf_text_col))\n",
    "    X_train_tfidf.append(X_train_tfidf_text_col)\n",
    "    \n",
    "    X_test_tfidf_text_col = load_npz(f'./dataset/text_embedding/{text_col}/emb_tfidf_test.npz')\n",
    "    X_test_tfidf_text_col = pd.DataFrame(dim.transform(X_test_tfidf_text_col))\n",
    "    X_test_tfidf.append(X_test_tfidf_text_col)\n",
    "X_train_tfidf = pd.concat(X_train_tfidf, axis=1)\n",
    "X_test_tfidf = pd.concat(X_test_tfidf, axis=1)\n",
    "\n",
    "# bert\n",
    "X_train_bert = []\n",
    "X_test_bert = []\n",
    "for text_col in text_cols:\n",
    "    dim = umap.UMAP(n_components=length)\n",
    "    \n",
    "    X_train_bert_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_bert_train.csv')\n",
    "    X_train_bert_text_col = pd.DataFrame(dim.fit_transform(X_train_bert_text_col))\n",
    "    X_train_bert.append(X_train_bert_text_col)\n",
    "    \n",
    "    X_test_bert_text_col = pd.read_csv(f'./dataset/text_embedding/{text_col}/emb_bert_test.csv')\n",
    "    X_test_bert_text_col = pd.DataFrame(dim.transform(X_test_bert_text_col))\n",
    "    X_test_bert.append(X_test_bert_text_col)\n",
    "X_train_bert = pd.concat(X_train_bert, axis=1)\n",
    "X_test_bert = pd.concat(X_test_bert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qxl9DOITdCMu"
   },
   "outputs": [],
   "source": [
    "# labels\n",
    "y_train = train_df['num_shares_bin']\n",
    "y_test = test_df['num_shares_bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "UGa_-IhYqURo"
   },
   "outputs": [],
   "source": [
    "# deep learning model classes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class LSTM:\n",
    "  def __init__(self):\n",
    "    self.model = None\n",
    "    self.le = LabelEncoder()\n",
    "\n",
    "  def fit(self, X_train, y_train):\n",
    "    y_train_encoded = self.le.fit_transform(y_train)\n",
    "    self.model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],1)),\n",
    "        layers.SpatialDropout1D(0.2),\n",
    "        layers.LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "        layers.LSTM(200, recurrent_dropout=0.2, return_sequences=True),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(300, activation='relu'),\n",
    "        layers.Dense(300, activation='relu'),\n",
    "        layers.Dense(3, activation='softmax')\n",
    "      ])\n",
    "    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    self.model.fit(X_train, y_train_encoded, batch_size=200, epochs=6, verbose=0)\n",
    "\n",
    "  def predict(self, X_test):\n",
    "    y_pred_encoded = self.model.predict(X_test).argmax(axis=-1)\n",
    "    y_pred = self.le.inverse_transform(y_pred_encoded)\n",
    "    return y_pred\n",
    "\n",
    "  def evaluate(self, X_test, y_test):\n",
    "    y_test_encoded = self.le.transform(y_test)\n",
    "    return self.model.evaluate(X_test, y_test_encoded, verbose=0)\n",
    "  \n",
    "\n",
    "class CNN:\n",
    "  def __init__(self):\n",
    "    self.model = None\n",
    "    self.le = LabelEncoder()\n",
    "\n",
    "  def fit(self, X_train, y_train):\n",
    "    y_train = self.le.fit_transform(y_train)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],1)),\n",
    "        layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='relu', padding='same'),\n",
    "        layers.GlobalMaxPooling1D(),\n",
    "        layers.Dense(500, activation='relu'), # FCNN\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(500, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(3, activation='softmax', name = 'Output') # output \n",
    "      ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, batch_size=200, epochs=6, verbose=0)\n",
    "    self.model = model\n",
    "\n",
    "  def predict(self, X_test):\n",
    "    y_pred = self.model.predict(X_test)\n",
    "    return self.le.inverse_transform(np.argmax(y_pred, axis=1))\n",
    "\n",
    "  def evaluate(self, X_test, y_test):\n",
    "    y_test = self.le.transform(y_test)\n",
    "    return self.model.evaluate(X_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "rOi1QQXgqa81"
   },
   "outputs": [],
   "source": [
    "# all embeddings\n",
    "embedding_dict = {\n",
    "    'Word2Vec: Skip-Gram' : {'X_train': X_train_sg, 'y_train': y_train, 'X_test': X_test_sg, 'y_test': y_test},\n",
    "    'Word2Vec: CBOW' : {'X_train': X_train_cbow, 'y_train': y_train, 'X_test': X_test_cbow, 'y_test': y_test},\n",
    "    'Doc2Vec: DBOW' : {'X_train': X_train_dbow, 'y_train': y_train, 'X_test': X_test_dbow, 'y_test': y_test},\n",
    "    'Doc2Vec: DM' : {'X_train': X_train_dm, 'y_train': y_train, 'X_test': X_test_dm, 'y_test': y_test},\n",
    "    'Doc2Vec: DBOW+DM' : {'X_train': X_train_dbow_dm, 'y_train': y_train, 'X_test': X_test_dbow_dm, 'y_test': y_test},\n",
    "    'TF-IDF w/ Bigram': {'X_train': X_train_tfidf, 'y_train': y_train, 'X_test': X_test_tfidf, 'y_test': y_test},\n",
    "    'Bert': {'X_train': X_train_bert, 'y_train': y_train, 'X_test': X_test_bert, 'y_test': y_test}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "25BfGdn9sABY"
   },
   "outputs": [],
   "source": [
    "models_to_evaluate = {\n",
    "    'LSTM': LSTM(),\n",
    "    'CNN': CNN(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=10000),\n",
    "    'SVM': SVC(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "def embeddings_evaluation(all_embeddings, model, model_name):\n",
    "    score_df = pd.DataFrame()\n",
    "    for name, embedding in all_embeddings.items():\n",
    "        try:\n",
    "            X_train = embedding.get('X_train')\n",
    "            y_train = embedding.get('y_train')\n",
    "            X_test = embedding.get('X_test')\n",
    "            y_test = embedding.get('y_test')\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_probs = model.predict(X_test)\n",
    "            y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "            if len(y_test) == 0:\n",
    "                acc, f1, precision, recall = 0, 0, 0, 0\n",
    "            else:\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        except ValueError:\n",
    "            X_train = embedding.get('X_train')\n",
    "            y_train = embedding.get('y_train')\n",
    "            X_test = embedding.get('X_test')\n",
    "            y_test = embedding.get('y_test')\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            if len(y_test) == 0:\n",
    "                acc, f1, precision, recall = 0, 0, 0, 0\n",
    "            else:\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                precision = precision_score(y_test, y_pred, average='weighted')\n",
    "                recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        new_row = {\n",
    "            'Model': model_name,\n",
    "            'Embedding': name, \n",
    "            'Accuracy': acc, \n",
    "            'Precision': precision,\n",
    "            'Recall': recall, \n",
    "            'F1-score': f1,\n",
    "        }\n",
    "\n",
    "        score_df = pd.concat([score_df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "    return score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OF39DbsvsFgI",
    "outputId": "30117597-8bb7-4a84-f47e-22be7646a90f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 6s 187ms/step\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 6s 188ms/step\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 8s 246ms/step\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 7s 211ms/step\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 11s 335ms/step\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 6s 174ms/step\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 8s 226ms/step\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 12s 346ms/step\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 12s 389ms/step\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 34s 282ms/step\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 9s 292ms/step\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 10s 298ms/step\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 12s 360ms/step\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "25/25 [==============================] - 11s 347ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 22ms/step\n",
      "25/25 [==============================] - 2s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 19ms/step\n",
      "25/25 [==============================] - 1s 25ms/step\n",
      "25/25 [==============================] - 1s 17ms/step\n",
      "25/25 [==============================] - 1s 16ms/step\n",
      "25/25 [==============================] - 1s 13ms/step\n",
      "25/25 [==============================] - 1s 27ms/step\n",
      "25/25 [==============================] - 1s 13ms/step\n",
      "25/25 [==============================] - 1s 15ms/step\n",
      "25/25 [==============================] - 1s 19ms/step\n",
      "25/25 [==============================] - 1s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PoYan1/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 13ms/step\n",
      "25/25 [==============================] - 1s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = pd.DataFrame()\n",
    "evaluation_result_dic = {}\n",
    "\n",
    "for model_name, model in models_to_evaluate.items():\n",
    "  scores = embeddings_evaluation(embedding_dict, model, model_name)\n",
    "  evaluation_result = pd.concat([evaluation_result, scores])\n",
    "  evaluation_result_dic[model_name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Word2Vec: Skip-Gram</td>\n",
       "      <td>0.349186</td>\n",
       "      <td>0.232504</td>\n",
       "      <td>0.349186</td>\n",
       "      <td>0.275927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Word2Vec: CBOW</td>\n",
       "      <td>0.374218</td>\n",
       "      <td>0.380117</td>\n",
       "      <td>0.374218</td>\n",
       "      <td>0.355789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Doc2Vec: DBOW</td>\n",
       "      <td>0.322904</td>\n",
       "      <td>0.312784</td>\n",
       "      <td>0.322904</td>\n",
       "      <td>0.217106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Doc2Vec: DM</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>0.117600</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>0.175140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Doc2Vec: DBOW+DM</td>\n",
       "      <td>0.327910</td>\n",
       "      <td>0.192975</td>\n",
       "      <td>0.327910</td>\n",
       "      <td>0.163940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>TF-IDF w/ Bigram</td>\n",
       "      <td>0.330413</td>\n",
       "      <td>0.109173</td>\n",
       "      <td>0.330413</td>\n",
       "      <td>0.164119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.326658</td>\n",
       "      <td>0.106706</td>\n",
       "      <td>0.326658</td>\n",
       "      <td>0.160864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Word2Vec: Skip-Gram</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.247812</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.261028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Word2Vec: CBOW</td>\n",
       "      <td>0.380476</td>\n",
       "      <td>0.384003</td>\n",
       "      <td>0.380476</td>\n",
       "      <td>0.373086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Doc2Vec: DBOW</td>\n",
       "      <td>0.372966</td>\n",
       "      <td>0.384626</td>\n",
       "      <td>0.372966</td>\n",
       "      <td>0.317516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Doc2Vec: DM</td>\n",
       "      <td>0.350438</td>\n",
       "      <td>0.421782</td>\n",
       "      <td>0.350438</td>\n",
       "      <td>0.296211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Doc2Vec: DBOW+DM</td>\n",
       "      <td>0.351690</td>\n",
       "      <td>0.375890</td>\n",
       "      <td>0.351690</td>\n",
       "      <td>0.274807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CNN</td>\n",
       "      <td>TF-IDF w/ Bigram</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.251774</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.303777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.409262</td>\n",
       "      <td>0.376828</td>\n",
       "      <td>0.409262</td>\n",
       "      <td>0.352587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Word2Vec: Skip-Gram</td>\n",
       "      <td>0.394243</td>\n",
       "      <td>0.394610</td>\n",
       "      <td>0.394243</td>\n",
       "      <td>0.393598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Word2Vec: CBOW</td>\n",
       "      <td>0.416771</td>\n",
       "      <td>0.415938</td>\n",
       "      <td>0.416771</td>\n",
       "      <td>0.415197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Doc2Vec: DBOW</td>\n",
       "      <td>0.359199</td>\n",
       "      <td>0.357406</td>\n",
       "      <td>0.359199</td>\n",
       "      <td>0.356569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Doc2Vec: DM</td>\n",
       "      <td>0.351690</td>\n",
       "      <td>0.350740</td>\n",
       "      <td>0.351690</td>\n",
       "      <td>0.324114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Doc2Vec: DBOW+DM</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.356694</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.341349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF-IDF w/ Bigram</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.351718</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.352740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.414268</td>\n",
       "      <td>0.414864</td>\n",
       "      <td>0.414268</td>\n",
       "      <td>0.414250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec: Skip-Gram</td>\n",
       "      <td>0.401752</td>\n",
       "      <td>0.395977</td>\n",
       "      <td>0.401752</td>\n",
       "      <td>0.396671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec: CBOW</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.416860</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.412766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec: DBOW</td>\n",
       "      <td>0.360451</td>\n",
       "      <td>0.356403</td>\n",
       "      <td>0.360451</td>\n",
       "      <td>0.355505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec: DM</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>0.308471</td>\n",
       "      <td>0.307885</td>\n",
       "      <td>0.288671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec: DBOW+DM</td>\n",
       "      <td>0.375469</td>\n",
       "      <td>0.393456</td>\n",
       "      <td>0.375469</td>\n",
       "      <td>0.315476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF w/ Bigram</td>\n",
       "      <td>0.397997</td>\n",
       "      <td>0.388316</td>\n",
       "      <td>0.397997</td>\n",
       "      <td>0.380099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.417909</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.412883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec: Skip-Gram</td>\n",
       "      <td>0.408010</td>\n",
       "      <td>0.408614</td>\n",
       "      <td>0.408010</td>\n",
       "      <td>0.405524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec: CBOW</td>\n",
       "      <td>0.429287</td>\n",
       "      <td>0.427742</td>\n",
       "      <td>0.429287</td>\n",
       "      <td>0.428190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec: DBOW</td>\n",
       "      <td>0.381727</td>\n",
       "      <td>0.375175</td>\n",
       "      <td>0.381727</td>\n",
       "      <td>0.371980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec: DM</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>0.354964</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>0.265862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec: DBOW+DM</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.379993</td>\n",
       "      <td>0.354193</td>\n",
       "      <td>0.267072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF w/ Bigram</td>\n",
       "      <td>0.390488</td>\n",
       "      <td>0.365353</td>\n",
       "      <td>0.390488</td>\n",
       "      <td>0.349660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Bert</td>\n",
       "      <td>0.415519</td>\n",
       "      <td>0.401296</td>\n",
       "      <td>0.415519</td>\n",
       "      <td>0.397739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model            Embedding  Accuracy  Precision    Recall  \\\n",
       "0                 LSTM  Word2Vec: Skip-Gram  0.349186   0.232504  0.349186   \n",
       "1                 LSTM       Word2Vec: CBOW  0.374218   0.380117  0.374218   \n",
       "2                 LSTM        Doc2Vec: DBOW  0.322904   0.312784  0.322904   \n",
       "3                 LSTM          Doc2Vec: DM  0.342929   0.117600  0.342929   \n",
       "4                 LSTM     Doc2Vec: DBOW+DM  0.327910   0.192975  0.327910   \n",
       "5                 LSTM     TF-IDF w/ Bigram  0.330413   0.109173  0.330413   \n",
       "6                 LSTM                 Bert  0.326658   0.106706  0.326658   \n",
       "0                  CNN  Word2Vec: Skip-Gram  0.354193   0.247812  0.354193   \n",
       "1                  CNN       Word2Vec: CBOW  0.380476   0.384003  0.380476   \n",
       "2                  CNN        Doc2Vec: DBOW  0.372966   0.384626  0.372966   \n",
       "3                  CNN          Doc2Vec: DM  0.350438   0.421782  0.350438   \n",
       "4                  CNN     Doc2Vec: DBOW+DM  0.351690   0.375890  0.351690   \n",
       "5                  CNN     TF-IDF w/ Bigram  0.382979   0.251774  0.382979   \n",
       "6                  CNN                 Bert  0.409262   0.376828  0.409262   \n",
       "0        Random Forest  Word2Vec: Skip-Gram  0.394243   0.394610  0.394243   \n",
       "1        Random Forest       Word2Vec: CBOW  0.416771   0.415938  0.416771   \n",
       "2        Random Forest        Doc2Vec: DBOW  0.359199   0.357406  0.359199   \n",
       "3        Random Forest          Doc2Vec: DM  0.351690   0.350740  0.351690   \n",
       "4        Random Forest     Doc2Vec: DBOW+DM  0.354193   0.356694  0.354193   \n",
       "5        Random Forest     TF-IDF w/ Bigram  0.354193   0.351718  0.354193   \n",
       "6        Random Forest                 Bert  0.414268   0.414864  0.414268   \n",
       "0  Logistic Regression  Word2Vec: Skip-Gram  0.401752   0.395977  0.401752   \n",
       "1  Logistic Regression       Word2Vec: CBOW  0.425532   0.416860  0.425532   \n",
       "2  Logistic Regression        Doc2Vec: DBOW  0.360451   0.356403  0.360451   \n",
       "3  Logistic Regression          Doc2Vec: DM  0.307885   0.308471  0.307885   \n",
       "4  Logistic Regression     Doc2Vec: DBOW+DM  0.375469   0.393456  0.375469   \n",
       "5  Logistic Regression     TF-IDF w/ Bigram  0.397997   0.388316  0.397997   \n",
       "6  Logistic Regression                 Bert  0.425532   0.417909  0.425532   \n",
       "0                  SVM  Word2Vec: Skip-Gram  0.408010   0.408614  0.408010   \n",
       "1                  SVM       Word2Vec: CBOW  0.429287   0.427742  0.429287   \n",
       "2                  SVM        Doc2Vec: DBOW  0.381727   0.375175  0.381727   \n",
       "3                  SVM          Doc2Vec: DM  0.342929   0.354964  0.342929   \n",
       "4                  SVM     Doc2Vec: DBOW+DM  0.354193   0.379993  0.354193   \n",
       "5                  SVM     TF-IDF w/ Bigram  0.390488   0.365353  0.390488   \n",
       "6                  SVM                 Bert  0.415519   0.401296  0.415519   \n",
       "\n",
       "   F1-score  \n",
       "0  0.275927  \n",
       "1  0.355789  \n",
       "2  0.217106  \n",
       "3  0.175140  \n",
       "4  0.163940  \n",
       "5  0.164119  \n",
       "6  0.160864  \n",
       "0  0.261028  \n",
       "1  0.373086  \n",
       "2  0.317516  \n",
       "3  0.296211  \n",
       "4  0.274807  \n",
       "5  0.303777  \n",
       "6  0.352587  \n",
       "0  0.393598  \n",
       "1  0.415197  \n",
       "2  0.356569  \n",
       "3  0.324114  \n",
       "4  0.341349  \n",
       "5  0.352740  \n",
       "6  0.414250  \n",
       "0  0.396671  \n",
       "1  0.412766  \n",
       "2  0.355505  \n",
       "3  0.288671  \n",
       "4  0.315476  \n",
       "5  0.380099  \n",
       "6  0.412883  \n",
       "0  0.405524  \n",
       "1  0.428190  \n",
       "2  0.371980  \n",
       "3  0.265862  \n",
       "4  0.267072  \n",
       "5  0.349660  \n",
       "6  0.397739  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "KF-r0SJJmkaj",
    "tdZVf43BOkek",
    "Zhshu1WLw4Z2",
    "22Wq-c_H9B2D",
    "cJAqxu-OaqWC"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
